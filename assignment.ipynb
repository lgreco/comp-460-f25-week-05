{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e50023",
   "metadata": {},
   "source": [
    "# Implementing a Multilayer Perceptron\n",
    "\n",
    "We discussed how a single perceptron, i.e., a neuron with just two weighted inputs $w_1 x_1$ and $w_2 x_2$ and a bias $b$ is basically a linear classifier in the plane $(x_1, x_2)$. Its output\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}1, \\quad\\text{if}\\ w_1 x_1 + w_2 x_2 + b > 0 \\\\ 0,\\quad\\text{otherwise}\\end{cases}\n",
    "$$\n",
    "\n",
    "separates the $(x_1, x_2)$ plane into two parts as shown below to the left. One half, above the blue line corresponds to $x_1$ and $x_2$ values that yield $y=1$. The other half is the values that yield $y=0$.\n",
    "\n",
    "![](/workspaces/comp-460-f25-week-05/images/linearClassifier.drawio.png)\n",
    "\n",
    "The right part of the figure above shows the values of the XOR function for $x_1, x_2 \\in\\{0,1\\}$. There is no single line that can bisect the $(x_1, x_2)$ plane into two parts containing the different values of the XOR function. To separate them, we need two lines. These two lines are effectively implemented by the neurons in the hidden layer below. Then their result is compined by the neuron in the output layer to create the separation between values of similar parity ($(0,0)$ and $(1,1)$) and values of different parity ($(1,0)$ and $(0,1)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630dca2e",
   "metadata": {},
   "source": [
    "![](/workspaces/comp-460-f25-week-05/images/XOR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22ce30c",
   "metadata": {},
   "source": [
    "In the notation below, a superscript labels the current layer. A single subscript labels the neuron. A double subscript labels a pair of neurons; the first subscript points to a neuron in the current layer and the second subscript to a neuron in the previous layer. In general,\n",
    "$\\textsf{relation}^{l}_{jk}$ describes a relation between the $j$-th neuron in the $l$-th layer and the $k$-th neuron in the $(l-1)$-th layer while $\\textsf{property}^l_j$ describes a property of the $j$-th neuron in the $l$-th layer.\n",
    "\n",
    "In mathematical typography, we use bolf fonts to denote a vector. In the classroom we use a little arrow over the same variable. For example, \n",
    "\n",
    "Using this notation, we can write the output of the first neuron in the second layer, $\\textsf{neuron}^2_1$ as: a vector $\\vec{x}$ written on whiteboard is the same as $\\mathbf x$ appearing in typography. For matrices the typographical convention is to print them in upper case letters and bold fonts. On the whiteboard, a matrix variable is implied from context.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  a^2_1 & = \\sigma( w^2_{11} x_1 + w^2_{12} x_2 + b^2_1 ) \\\\ & =\n",
    "  \\sigma\\left(\n",
    "    \\begin{bmatrix} w^2_{11} & w^2_{12}\\end{bmatrix} \\cdot\n",
    "    \\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix} + b^2_1 \\right) \\\\ &=\n",
    "    \\sigma\\left( \\mathbf W^2_1\\cdot\\mathbf x + b^2_1\\right) \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Similarly, the output of the second neuron in the second layer, $\\textsf{neuron}^2_2$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  a^2_2 & = \\sigma\\left( \\mathbf W^2_2\\cdot\\mathbf x + b^2_2 \\right) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "  \\begin{bmatrix}a^2_1 \\\\ \\\\a^2_2 \\end{bmatrix} & =\n",
    "  \\sigma \\left(\n",
    "    \\begin{bmatrix}\\mathbf W^2_1 \\\\ \\\\ \\mathbf W^2_2 \\end{bmatrix} \\cdot\\mathbf x +\n",
    "    \\begin{bmatrix}b^2_1 \\\\ \\\\ b^2_2 \\end{bmatrix}\n",
    "  \\right) \\Rightarrow \\\\ \\\\\n",
    "  \\mathbf a^2 & = \\sigma \\left(\\mathbf W^2 \\cdot \\mathbf x + \\mathbf b^2 \\right) \\\\\n",
    "  & = \\sigma\\left(\\mathbf z^2\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "As we compact the notation, $\\mathbf a^2$ is the output vector for the hidden layer in response to $\\mathbf z^2=\\mathbf W^2 \\cdot \\mathbf x + \\mathbf b^2 $.\n",
    "Matrix $\\mathbf W^2$ contains the input weights for that layer.\n",
    "\n",
    "$$\n",
    "\\mathbf W^2 =\n",
    "\\begin{bmatrix}\n",
    "  w^2_{11} & w^2_{12} \\\\\n",
    "  w^2_{21} & w^2_{22}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68b275c",
   "metadata": {},
   "source": [
    "For the neuron in the third layer, the output is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a^3_1 & = \\sigma \\left( a^2_1 w^3_{11} + a^2_2 w^3_{12} +b^3_1 \\right) \\\\\n",
    "      & = \\sigma \\left( \\mathbf a^2 \\cdot \\mathbf W^3 + b^3_1 \\right) \\\\\n",
    "      & = \\sigma \\left( \\sigma(\\mathbf z^2) \\cdot \\mathbf W^3 + b^3_1 \\right) \\\\\n",
    "      & =  \\sigma \\left( \\sigma(\\mathbf W^2 \\cdot \\mathbf x + \\mathbf b^2) \\cdot \\mathbf W^3 + b^3_1 \\right)\n",
    "      &\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In vector form, $\\mathbf a^3 = \\sigma \\left( \\sigma\\left(\\mathbf W^2 \\cdot \\mathbf x + \\mathbf b^2\\right) \\cdot \\mathbf W^3 + \\mathbf b^3 \\right)$,\n",
    "where $\\mathbf a^3 = \\begin{bmatrix} a^3_1 \\end{bmatrix}$ and $\\mathbf b^3 = \\begin{bmatrix} b^3_1 \\end{bmatrix}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4548e",
   "metadata": {},
   "source": [
    "Consider the following weights and biases for the second and third layers:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf W^2 &= \\begin{bmatrix} 20 & 20 \\\\ -20 & -20  \\end{bmatrix}\\qquad\n",
    "& \\mathbf W^3 &= \\begin{bmatrix} 20 & 20  \\end{bmatrix}  \\\\\n",
    "\\mathbf b^2 &= \\begin{bmatrix}  -10 \\\\ 30  \\end{bmatrix} \\qquad\n",
    "& \\mathbf b^3 &= \\begin{bmatrix} -30  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df723506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "# Sigmoid activation\n",
    "def sigmoid(z: float) -> float:\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "\n",
    "# Forward pass through fixed weights\n",
    "def mystery(x1: int, x2: int) -> int:\n",
    "    # Hard-coded weights for hidden layer (2 neurons)\n",
    "\n",
    "    w2 = [[20, 20], [-20, -20]]  # neuron 1  # neuron 2\n",
    "    b2 = [-10, 30]  # biases for hidden neurons\n",
    "\n",
    "    # Hidden activations\n",
    "    hidden = []\n",
    "    for j in range(2):\n",
    "        z = w2[j][0] * x1 + w2[j][1] * x2 + b2[j]\n",
    "        hidden.append(sigmoid(z))\n",
    "\n",
    "    # Output neuron combines them: essentially hidden[0] - hidden[1]\n",
    "    w3 = [20, 20]\n",
    "    b3 = -30\n",
    "\n",
    "    z_out = w3[0] * hidden[0] + w3[1] * hidden[1] + b3\n",
    "    output = sigmoid(z_out)\n",
    "\n",
    "    return round(output)  # round to 0 or 1\n",
    "\n",
    "\n",
    "# Test XOR\n",
    "print(f\"\\na  b   mystery\\n-------------\")\n",
    "for a in [0, 1]:\n",
    "    for b in [0, 1]:\n",
    "        print(f\"{a}  {b}     {mystery(a,b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90df8129",
   "metadata": {},
   "source": [
    "# Your assignment\n",
    "\n",
    "# Reading\n",
    "\n",
    "- **PDF:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938786b2",
   "metadata": {},
   "source": [
    "## The sigmoid function\n",
    "\n",
    "The sigmoid function\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "is a smoother function that has similar behavior to the step function. For large values of $z$, $\\sigma(z) \\rightarrow 1$ (and for small values of $z$, $\\sigma(z) \\rightarrow 0$). For any value inbetween, $\\sigma(z)$ has a smoother behavior that the step function and, more importantly, can be differentiated:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d}{dz}\\sigma(z) = \\sigma(z)\\left(1-\\sigma(z)\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The derivative above is obtained with the chain rule for $\\sigma(z) = f(u(z))$ where $u(z) = 1+e^{-z}$ and $f(u) = u^{-1}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d}{dz}\\sigma(z) & = \\frac{d}{dz} f(u(z)) \\\\\n",
    "   & = \\frac{df}{du}\\frac{du}{dz} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "with $df/du = -u^{-2}$ and $du/dz = -e^{z}$, so that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\frac{df}{du}\\frac{du}{dz} & = (-(1+e^{-z}))^{-2} (-e^{-z}) \\\\\n",
    "                             & = \\frac{e^{-z}}{(1+e^{-z})^{-2}} \\\\\n",
    "                             & = \\left(\\frac{1}{1+e^{-z}}\\right) \\left(\\frac{e^{-z}}{1+e^{-z}}\\right) \\\\\n",
    "                             &  = \\left(\\frac{1}{1+e^{-z}}\\right)   \\left(\\frac{1+e^{-z}-1}{1+e^{-z}}\\right) \\\\\n",
    "                            &  = \\left(\\frac{1}{1+e^{-z}}\\right)   \\left(\\frac{1+e^{-z}}{1+e^{-z}}-\\frac{1}{1+e^{-z}}\\right) \\\\\n",
    "                            & =  \\left(\\frac{1}{1+e^{-z}}\\right)   \\left(1-\\frac{1}{1+e^{-z}}\\right) \\\\\n",
    "                            & = \\sigma(z)(1-\\sigma(z))\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def sigmoid(z: float) -> float:\n",
    "    return 1 / (1 + math.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(z: float) -> float:\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee09233",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Backpropagation Example (2-2-1 Network, XOR input)\n",
    "\n",
    "We demonstrate **one training step** of backpropagation for input $(x_1,x_2)=(1,0)$ with target $y=1$.\n",
    "\n",
    "---\n",
    "\n",
    "## Forward pass\n",
    "\n",
    "$$\n",
    "z_1 = w_{11}x_1+w_{12}x_2+b_1 = 0.10, \\quad a_1 = \\sigma(z_1)\\approx 0.524979, \\\\\n",
    "z_2 = w_{21}x_1+w_{22}x_2+b_2 = -0.10, \\quad a_2 = \\sigma(z_2)\\approx 0.475021, \\\\\n",
    "z_3 = v_1a_1+v_2a_2+b_3 = 0.10, \\quad \\hat y = \\sigma(z_3)\\approx 0.524979.\n",
    "$$\n",
    "\n",
    "Loss:\n",
    "$$\n",
    "L = \\tfrac12(y-\\hat y)^2 \\approx 0.112822.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Backward pass\n",
    "\n",
    "Output error term:\n",
    "$$\n",
    "\\delta^{(3)} = (\\hat y - y)\\,\\hat y(1-\\hat y) \\approx -0.118459.\n",
    "$$\n",
    "\n",
    "Hidden error terms:\n",
    "$$\n",
    "\\delta^{(1)} \\approx -0.002954, \\qquad \\delta^{(2)} \\approx -0.002954.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Gradients\n",
    "\n",
    "Output layer:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial v_1} \\approx -0.062188, \\quad\n",
    "\\frac{\\partial L}{\\partial v_2} \\approx -0.056270, \\quad\n",
    "\\frac{\\partial L}{\\partial b_3} \\approx -0.118459.\n",
    "$$\n",
    "\n",
    "Hidden layer (since $x_2=0$):\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{11}} \\approx -0.002954, \\quad\n",
    "\\frac{\\partial L}{\\partial w_{12}} = 0, \\quad\n",
    "\\frac{\\partial L}{\\partial b_1} \\approx -0.002954,\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{21}} \\approx -0.002954, \\quad\n",
    "\\frac{\\partial L}{\\partial w_{22}} = 0, \\quad\n",
    "\\frac{\\partial L}{\\partial b_2} \\approx -0.002954.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Weight updates (learning rate $\\eta=0.5$)\n",
    "\n",
    "$$\n",
    "v_1 \\leftarrow 0.10 - 0.5(-0.062188) = 0.131094, \\\\\n",
    "v_2 \\leftarrow 0.10 - 0.5(-0.056270) = 0.128135, \\\\\n",
    "b_3 \\leftarrow 0.00 - 0.5(-0.118459) = 0.059229, \\\\\n",
    "w_{11} \\leftarrow 0.10 - 0.5(-0.002954) = 0.101477, \\\\\n",
    "w_{12} \\leftarrow 0.20, \\quad\n",
    "b_1 \\leftarrow 0.001477, \\\\\n",
    "w_{21} \\leftarrow -0.10 - 0.5(-0.002954) = -0.098523, \\\\\n",
    "w_{22} \\leftarrow 0.10, \\quad\n",
    "b_2 \\leftarrow 0.001477.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Improvement\n",
    "\n",
    "Forward again with updated parameters:\n",
    "\n",
    "$$\n",
    "\\hat y \\approx 0.547137, \\quad L \\approx 0.102543.\n",
    "$$\n",
    "\n",
    "âœ… Prediction moved closer to target and loss decreased.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
